#!/usr/bin/env python3
"""
Script d'analyse de vidÃ©os utilisant YOLOv8 prÃ©-entraÃ®nÃ© sur diffÃ©rents datasets
Supporte COCO, Open Images V7, et modÃ¨les personnalisÃ©s
"""

import cv2 # pour la lecture des vidÃ©os
import torch # pour le deep learning
from ultralytics import YOLO # pour le modÃ¨le YOLOv8
import matplotlib.pyplot as plt # pour la visualisation
import matplotlib.patches as patches # pour les boÃ®tes de dÃ©tection
from PIL import Image # pour la manipulation d'images
import numpy as np # pour les calculs numÃ©riques
import os # pour le systÃ¨me de fichiers
import time # pour mesurer les performances

_model_cache = {}

# Informations sur les datasets supportÃ©s
DATASET_INFO = {
    'coco': {
        'name': 'COCO',
        'full_name': 'Common Objects in Context',
        'classes': None, # 80 classes
        'num_classes': 80,
        'website': 'https://cocodataset.org/',
        'description': 'Dataset gÃ©nÃ©raliste avec 80 classes d\'objets communs'
    },
    'oiv7': {
        'name': 'Open Images V7',
        'full_name': 'Open Images Dataset V7',
        'classes': None,  # 600 classes
        'num_classes': 600,
        'website': 'https://storage.googleapis.com/openimages/web/index.html',
        'description': 'Dataset large avec 600 classes d\'objets variÃ©s'
    },
    'seg': {
        'name': 'Segmentation',
        'full_name': 'YOLOv8 Segmentation (COCO)',
        'classes': None,  # 80 classes COCO
        'num_classes': 80,
        'website': 'https://cocodataset.org/',
        'description': 'Segmentation d\'objets avec masques de pixels prÃ©cis'
    },
    'custom-trained': {
        'name': 'ModÃ¨le personnalisÃ©',
        'full_name': 'ModÃ¨le personnalisÃ©',
        'classes': None,  # 4 classes
        'num_classes': 4,
        'website': 'https://github.com/ultralytics/ultralytics',
        'description': 'ModÃ¨le YOLOv8 fine-tunÃ© sur dataset Aerial Cars'
    }
}

def detect_dataset_type(model_path):
    """
    DÃ©tecte le type de dataset basÃ© sur le nom du modÃ¨le
    
    Args:
        model_path (str): Chemin vers le modÃ¨le
        
    Returns:
        str: Type de dataset ('coco', 'oiv7', 'seg', 'custom-trained')
    """
    if 'best.pt' in model_path or 'last.pt' in model_path:
        return 'custom-trained'
    elif 'seg' in model_path.lower():
        if model_path[6] == 'n':
            return 'n-seg'
        elif model_path[6] == 's':
            return 's-seg'
        elif model_path[6] == 'm':
            return 'm-seg'
        elif model_path[6] == 'l':
            return 'l-seg'
        elif model_path[6] == 'x':
            return 'x-seg'
    elif 'oiv7' in model_path.lower():
        if model_path[6] == 'n':
            return 'n-oiv7'
        elif model_path[6] == 's':
            return 's-oiv7'
        elif model_path[6] == 'm':
            return 'm-oiv7'
        elif model_path[6] == 'l':
            return 'l-oiv7'
        elif model_path[6] == 'x':
            return 'x-oiv7'
    else:
        if model_path[6] == 'n':
            return 'n-coco'
        elif model_path[6] == 's':
            return 's-coco'
        elif model_path[6] == 'm':
            return 'm-coco'
        elif model_path[6] == 'l':
            return 'l-coco'
        elif model_path[6] == 'x':
            return 'x-coco'


def get_model(model_path):
    """
    RÃ©cupÃ¨re un modÃ¨le depuis le cache ou le charge s'il n'existe pas
    
    Args:
        model_path (str): Chemin vers le modÃ¨le YOLOv8
        
    Returns:
        YOLO: ModÃ¨le YOLOv8 chargÃ©
    """
    if model_path not in _model_cache:
        print(f"ğŸ”„ Chargement du modÃ¨le {model_path}...")
        try:
            _model_cache[model_path] = YOLO(model_path)
            print(f"âœ… ModÃ¨le {model_path} chargÃ© avec succÃ¨s")
        except Exception as e:
            print(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            raise
    else:
        print(f"â™»ï¸  Utilisation du modÃ¨le {model_path} depuis le cache")
    
    return _model_cache[model_path]

def analyze_video(video_path, model_path, seuil_conf):
    """
    Analyse une vidÃ©o avec YOLOv8 prÃ©-entraÃ®nÃ© sur diffÃ©rents datasets
    
    Args:
        video_path (str): Chemin vers la vidÃ©o Ã  analyser
        model_path (str): Chemin vers le modÃ¨le YOLOv8
        seuil_conf (float): Seuil de confiance minimum
    """
    
    # DÃ©tecter le type de dataset
    dataset_type = detect_dataset_type(model_path)
    if dataset_type == 'custom-trained':
        dataset_info = DATASET_INFO[dataset_type]
    else:
        dataset_info = DATASET_INFO[dataset_type[2:]]
    
    # VÃ©rifier si la vidÃ©o existe
    try:
        cap = cv2.VideoCapture(video_path)
        if cap is None:
            print(f"âŒ Erreur: Impossible de charger la vidÃ©o {video_path}")
            return
    except Exception as e:
        print(f"âŒ Erreur lors du chargement de la vidÃ©o: {e}")
        cap.release()
        return
    
    # RÃ©cupÃ©rer le modÃ¨le (depuis le cache ou le charger)
    try:
        model = get_model(model_path)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
        cap.release()
        return
    
    print(f"\nğŸ¬ ANALYSE VIDÃ‰O YOLOV8 + DATASET {dataset_info['name'].upper()}")
    print("=" * 50)

    # Obtenir les propriÃ©tÃ©s de la vidÃ©o
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    print(f"ğŸ¥ PropriÃ©tÃ©s de la vidÃ©o:")
    print(f"   ğŸ“ RÃ©solution: {width}x{height}")
    print(f"   â±ï¸  FPS: {fps}")
    print(f"   ğŸ“Š Frames totales: {total_frames}")
    print(f"   â° DurÃ©e: {duration:.1f} secondes")
    print(f"   ğŸ¯ Seuil de confiance: {seuil_conf}")
    print()
    
    # Statistiques
    frame_count = 0
    total_detections = 0
    detection_history = []
    start_time = time.time()
    
    # Couleurs pour les boÃ®tes de dÃ©tection et masques
    colors = plt.cm.tab10(np.linspace(0, 1, 10))
    
    print("ğŸš€ DÃ©but de l'analyse...")
    
    try:
        # CrÃ©er une fenÃªtre pour afficher la vidÃ©o
        cv2.namedWindow("Analyse Video YOLOv8", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("Analyse Video YOLOv8", 2560, 1440)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Afficher le progrÃ¨s
            if frame_count % 30 == 0:  # Toutes les 30 frames
                progress = (frame_count / total_frames) * 100
                print(f"ğŸ“Š ProgrÃ¨s: {progress:.1f}% ({frame_count}/{total_frames} frames)")
            
            # Effectuer la dÃ©tection sur la frame
            try:
                results = model(frame, conf=seuil_conf)
                result = results[0]
                
                # Compter les dÃ©tections
                frame_detections = 0
                if result.boxes is not None and result.masks is None:
                    frame_detections = len(result.boxes)
                    total_detections += frame_detections
                
                detection_history.append(frame_detections)
                
                # Dessiner les dÃ©tections sur la frame
                annotated_frame = frame.copy()
                
                # Dessiner les masques s'ils existent
                if result.masks is not None:
                    for i, mask in enumerate(result.masks):
                        try:
                            mask_array = mask.data[0].cpu().numpy()
                            
                            # VÃ©rifications de sÃ©curitÃ©
                            if mask_array.size == 0 or mask_array.shape[0] == 0 or mask_array.shape[1] == 0:
                                continue
                            
                            # Redimensionner le masque seulement si nÃ©cessaire
                            if mask_array.shape != (height, width):
                                if mask_array.shape[0] > 0 and mask_array.shape[1] > 0:
                                    mask_resized = cv2.resize(mask_array.astype(np.uint8), 
                                                            (width, height))
                                else:
                                    continue
                            else:
                                mask_resized = mask_array.astype(np.uint8)
                            
                            # CrÃ©er un masque colorÃ©
                            color = colors[i % len(colors)]
                            colored_mask = np.zeros((height, width, 3), dtype=np.uint8)
                            colored_mask[mask_resized == 1] = [int(c * 255) for c in color[:3]]
                            
                            # Dessiner uniquement les contours du masque
                            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                            cv2.drawContours(annotated_frame, contours, -1, [int(c * 255) for c in color[:3]], 2)

                            # Calculer le centre du contour pour placer le label
                            for contour in contours:
                                M = cv2.moments(contour)
                                if M["m00"] != 0:
                                    cx = int(M["m10"] / M["m00"])
                                    cy = int(M["m01"] / M["m00"])
                                    
                                    # CORRECTION : Utiliser result.boxes pour obtenir la classe et la confiance
                                    if result.boxes is not None and i < len(result.boxes):
                                        box = result.boxes[i]
                                        class_id = int(box.cls[0].cpu().numpy())
                                        class_name = result.names[class_id]
                                        confidence = box.conf[0].cpu().numpy()
                                        
                                        # Ajouter le label avec fond colorÃ©
                                        label = f"{class_name} {confidence:.2f}"
                                        (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                                        
                                        # Dessiner le fond du label
                                        cv2.rectangle(annotated_frame, 
                                                    (cx - label_w//2, cy - label_h - 10),
                                                    (cx + label_w//2, cy), 
                                                    [int(c * 255) for c in color[:3]], 
                                                    -1)
                                        
                                        # Dessiner le texte
                                        cv2.putText(annotated_frame, 
                                                  label, 
                                                  (cx - label_w//2, cy - 5),
                                                  cv2.FONT_HERSHEY_SIMPLEX, 
                                                  0.6, 
                                                  (255, 255, 255), 
                                                  2)
                            
                        except Exception as e:
                            print(f"âš ï¸  Erreur masque {i}: {e}")
                            continue
                
                # Dessiner les boÃ®tes de dÃ©tection
                if result.boxes is not None and result.masks is None:
                    for i, box in enumerate(result.boxes):
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                        confidence = box.conf[0].cpu().numpy()
                        class_id = int(box.cls[0].cpu().numpy())
                        class_name = result.names[class_id]
                        
                        # Dessiner la boÃ®te
                        color = colors[i % len(colors)]
                        color_bgr = [int(c * 255) for c in color[:3]]
                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color_bgr, 2)
                        
                        # Ajouter le label avec fond colorÃ©
                        label = f"{class_name} {confidence:.2f}"
                        (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                        cv2.rectangle(annotated_frame, (x1, y1-label_h-10), (x1+label_w, y1), color_bgr, -1)
                        cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Ajouter des informations sur la frame avec fond colorÃ©
                info_text = f"Frame: {frame_count}/{total_frames} | Detections: {frame_detections}"
                (text_w, text_h), _ = cv2.getTextSize(info_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
                cv2.rectangle(annotated_frame, (5, 5), (15+text_w, 35+text_h), (0, 100, 0), -1)
                cv2.putText(annotated_frame, info_text, (10, 30), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                # Redimensionner pour un affichage plus grand
                max_width = 2560
                max_height = 1440
                if width > max_width or height > max_height:
                    scale = min(max_width/width, max_height/height)
                    new_width = int(width * scale)
                    new_height = int(height * scale)
                    annotated_frame = cv2.resize(annotated_frame, (new_width, new_height))
                else:
                    # Agrandir la vidÃ©o si elle est plus petite que la taille maximale
                    scale = min(max_width/width, max_height/height)
                    if scale > 1:
                        new_width = int(width * scale)
                        new_height = int(height * scale)
                        annotated_frame = cv2.resize(annotated_frame, (new_width, new_height))
                
                # Afficher la frame
                cv2.imshow('Analyse Video YOLOv8', annotated_frame)
                
                # Attendre 1ms et vÃ©rifier si l'utilisateur veut quitter (touche 'q')
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nâ¹ï¸  Analyse interrompue par l'utilisateur")
                    break
                
            except Exception as e:
                print(f"âŒ Erreur lors de l'analyse de la frame {frame_count}: {e}")
                continue
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Analyse interrompue par l'utilisateur")
    
    finally:
        # Nettoyer
        cap.release()       
        
        # Calculer les statistiques finales
        end_time = time.time()
        processing_time = end_time - start_time
        avg_fps = frame_count / processing_time if processing_time > 0 else 0
        
        print(f"\nğŸ“Š RÃ‰SULTATS DE L'ANALYSE")
        print("=" * 50)
        print(f"ğŸ¬ Frames traitÃ©es: {frame_count}")
        print(f"ğŸ” Total dÃ©tections: {total_detections}")
        print(f"ğŸ“ˆ Moyenne dÃ©tections/frame: {total_detections/frame_count:.2f}")
        print(f"â±ï¸  Temps de traitement: {processing_time:.1f} secondes")
        print(f"âš¡ FPS de traitement: {avg_fps:.1f}")
        print(f"ğŸ¯ FPS vidÃ©o original: {fps}")
        
        print(f"\nğŸ Analyse terminÃ©e!")
        print()

def show_dataset_info(dataset_type):
    """
    Affiche les informations sur le dataset utilisÃ©
    
    Args:
        dataset_type (str): Type de dataset ('coco', 'oiv7', 'seg')
    """

    # Gestion du modÃ¨le personnalisÃ©
    if dataset_type == 'custom-trained':
        print()
        print(f"ğŸ“Š INFORMATIONS MODÃˆLE PERSONNALISÃ‰")
        print("=" * 50)
        print(f"ğŸ“š Dataset: ModÃ¨le entraÃ®nÃ© personnalisÃ©")
        print(f"ğŸ”¢ Nombre de classes: 4")
        print(f"ğŸŒ Type: DÃ©tection de vÃ©hicules aÃ©riens")
        print(f"ğŸ“ Description: ModÃ¨le YOLOv8 fine-tunÃ© sur dataset Aerial Cars")
        print()
        print("ğŸ·ï¸ Classes dÃ©tectÃ©es:")
        print("   ğŸš— car")
        print("   ğŸš› truck") 
        print("   ğŸšŒ bus")
        print("   ğŸš van")
        print()
        return

    # Gestion des datasets prÃ©-entraÃ®nÃ©s
    dataset_info = DATASET_INFO[dataset_type[2:]]
    
    print()
    print(f"ğŸ“Š INFORMATIONS DATASET {dataset_info['name'].upper()}")
    print("=" * 50)
    print(f"ğŸ“š Dataset: {dataset_info['full_name']}")
    print(f"ğŸ”¢ Nombre de classes: {dataset_info['num_classes']}")
    print(f"ğŸŒ Site officiel: {dataset_info['website']}")
    print(f"ğŸ“ Description: {dataset_info['description']}")
    print()
    
    if dataset_type == 'coco':
        print("ğŸ·ï¸ CatÃ©gories principales:")
        print("   ğŸ‘¥ Personnes: person")
        print("   ğŸš— VÃ©hicules: car, bus, truck, motorcycle, bicycle, etc.")
        print("   ğŸ• Animaux: cat, dog, horse, cow, sheep, bird, etc.")
        print("   ğŸ  Objets: chair, table, laptop, cell phone, book, etc.")
        print("   ğŸ Nourriture: apple, banana, pizza, cake, etc.")
        print("   âš½ Sports: sports ball, tennis racket, baseball bat, etc.")
    elif dataset_type == 'oiv7':
        print("ğŸ·ï¸ CatÃ©gories principales:")
        print("   ğŸ‘¥ Personnes et parties du corps")
        print("   ğŸš— VÃ©hicules de tous types")
        print("   ğŸ• Animaux domestiques et sauvages")
        print("   ğŸ  Objets du quotidien")
        print("   ğŸ Nourriture et boissons")
        print("   âš½ Sports et loisirs")
        print("   ğŸ¨ Art et culture")
        print("   ğŸŒ Nature et environnement")
    elif dataset_type == 'seg':
        print("ğŸ¨ FonctionnalitÃ©s de segmentation:")
        print("   ğŸ” DÃ©tection d'objets + masques de pixels")
        print("   ğŸ“ Contours prÃ©cis des objets")
        print("   ğŸ¯ 80 classes COCO avec segmentation")
        print("   ğŸ’¡ IdÃ©al pour l'Ã©dition de vidÃ©os et la robotique")
    print()

def main():
    """Fonction principale"""    
    # Exemples de modÃ¨les pour diffÃ©rents datasets :
    model_paths = {
        # ModÃ¨les de DÃ‰TECTION
        'n-coco': "yolov8n.pt",           
        's-coco': "yolov8s.pt",           
        'm-coco': "yolov8m.pt",           
        'l-coco': "yolov8l.pt",           
        'x-coco': "yolov8x.pt",           
        
        # ModÃ¨les de SEGMENTATION
        'n-seg': "yolov8n-seg.pt",
        's-seg': "yolov8s-seg.pt", 
        'm-seg': "yolov8m-seg.pt",
        'l-seg': "yolov8l-seg.pt",
        'x-seg': "yolov8x-seg.pt",
        
        # ModÃ¨les Open Images V7
        'n-oiv7': "yolov8n-oiv7.pt",      
        's-oiv7': "yolov8s-oiv7.pt",      
        'm-oiv7': "yolov8m-oiv7.pt",      
        'l-oiv7': "yolov8l-oiv7.pt",      
        'x-oiv7': "yolov8x-oiv7.pt",      

        # ModÃ¨les personnalisÃ©s
        'custom-trained': "runs/train6/weights/best.pt",
    }
    
    # Choisir le dataset Ã  utiliser
    dataset_choice = 'custom-trained' 
    model_path = model_paths[dataset_choice]

    # Configuration - Changez ces valeurs pour tester diffÃ©rents datasets
    video_path = "Videos/video2.mp4"

    # Choisir le seuil de confiance
    seuil_conf = 0.8
    
    # Afficher les informations sur le dataset
    show_dataset_info(dataset_choice)
    
    # Lancer l'analyse
    analyze_video(video_path, model_path, seuil_conf)

if __name__ == "__main__":
    main()
